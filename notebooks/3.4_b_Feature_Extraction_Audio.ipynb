{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e319dd7",
   "metadata": {},
   "source": [
    "## **Extracción de Características Auditivas (Acoustic Embeddings)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6192d",
   "metadata": {},
   "source": [
    "En este notebook abordamos la fase de Feature Extraction para la modalidad de audio. A diferencia de la transcripción de texto (donde importa qué se dice), en el análisis acústico nos centramos en cómo se dice. Para capturar esta información paralingüística, aplicaremos dos estrategias, extrayendo dos tipos de características complementarias, siguiendo un flujo de trabajo similar al notebook **3.4_a**:\n",
    "\n",
    "1. Representaciones de Aprendizaje Profundo (Deep Learning): **Wav2Vec 2.0**.\n",
    "    * Este modelo, desarrollado por Facebook AI, aprende representaciones contextuales directamente de la forma de onda del audio sin procesar. Basado en Transformers, utiliza mecanismos de auto-atención (self-attention) para capturar dependencias temporales complejas. Al haber sido pre-entrenado en miles de horas de audio no etiquetado, genera embeddings que encapsulan información fonética, prosódica y emocional de alta calidad. Es el estándar de facto (SOTA). Se ha seleccionado el modelo **Base** (768) en lugar de **Large** (1024) ya que, al igual que el vídeo, los audios en MELD e IEMOCAP son muy cortos, por tanto un vector de 1024 dimensiones hubiera memorizado el ruido de fondo o particularidades del locutor en lugar de características emocionales generales. La versión **Base** ofrece una representación más generalista y robusta para la detección de estrés.\n",
    "\n",
    "        - **SALIDA**: Vectores contextuales de **768** dimensiones.\n",
    "\n",
    "2. Características *Artesanales* (*Hand-crafted Features*): **MFCCs + Energía**.\n",
    "    * Como línea base (baseline) robusta y computacionalmente ligera, extraemos características espectrales y prosódicas utilizando la librería `librosa`. \n",
    "    Los **MFCCs** (**Mel-frequency cepstral coefficients**) son coeficientes que representan el timbre de la voz tal y como lo percibe el oído humano. Son el estándar histórico en el reconocimiento de emociones. La **Energía** (**RMS**) y **ZCR** (*Zero-Crossing Rate*) son indicadores directos de la intensidad sonora (volumen) y la velocidad/\"rugosidad\" del habla, variables que suelen aumentar en estados de estrés.\n",
    "        - **MFCCs**: Extraeremos **13** coeficientes. Se seleccionan los primeros 13 ya que modelan eficazmente la envolvente del tracto vocal, descartando el ruido de alta frecuencia.\n",
    "        - **Energía** (**RMS**): La raíz cuadrática media (*Root Mean Square*) es un indicador directo de la intensidad sonora. Responde a la pregunta de: *¿El sujeto grita o susurra?*. Extraeremos **1 RMS**.\n",
    "        - **ZCR**: La tasa de cruces por cero mide la frecuencia de cambio de signo en la señal. Responde a la pregunta de: *¿Es una voz limpia, o hay ruido/rugosidad?*. Extraeremos **1 ZCR**.\n",
    "    \n",
    "        - **SALIDA**: Vectores de **15** dimensiones (13+1+1).\n",
    "\n",
    "A diferencia de la extracción de vídeo (donde fijamos un número fijo de **32 frames**), en audio, la dimensión temporal (*Time Steps*) será variable en función de la duración de cada frase. Con **Wav2Vec 2.0** generamos un vector de características cada 20 ms aproximadamente, con dimensiones $(T_w, 768)$.Con la alternativa *Hand-crafted* generamos un vector de características en función del tamaño de ventana y salto (*hop length*) definidos, típicamente cada 32 ms, con dimensiones de $(T_h, 15)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4629202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Carga previa de todas las librerías y paquetes necesarios\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from tqdm import tqdm  # Para barra de progreso\n",
    "from google.colab import drive\n",
    "import random\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# Silencionamos los warnings:\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Configuración de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bddff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Se monta Drive para acceder a los datos y guardar los resultados:\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ba7e8",
   "metadata": {},
   "source": [
    "Traemos los datos al disco del servidor de Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40af752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copiando /content/drive/MyDrive/Proyecto_TFG_Data/MELD_Audio.zip a /content/data.\n",
      "Copiando /content/drive/MyDrive/Proyecto_TFG_Data/IEMOCAP_Audio.zip a /content/data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fb28e3af-2dca-4ba1-8c0e-1a40a7dbbe8d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>video_path</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "      <th>target_stress</th>\n",
       "      <th>dataset_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_dia0_utt0</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt0.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt0.wav</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>5.672333</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_dia0_utt1</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt1.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt1.wav</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>1.501500</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_dia0_utt2</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt2.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt2.wav</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>2.919583</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_dia0_utt3</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt3.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt3.wav</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>2.752750</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_dia0_utt4</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt4.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt4.wav</td>\n",
       "      <td>My duties? All right.</td>\n",
       "      <td>6.464792</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb28e3af-2dca-4ba1-8c0e-1a40a7dbbe8d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fb28e3af-2dca-4ba1-8c0e-1a40a7dbbe8d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fb28e3af-2dca-4ba1-8c0e-1a40a7dbbe8d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Utterance_ID Dialogue_ID                  video_path  \\\n",
       "0  train_dia0_utt0           0  train_splits/dia0_utt0.mp4   \n",
       "1  train_dia0_utt1           0  train_splits/dia0_utt1.mp4   \n",
       "2  train_dia0_utt2           0  train_splits/dia0_utt2.mp4   \n",
       "3  train_dia0_utt3           0  train_splits/dia0_utt3.mp4   \n",
       "4  train_dia0_utt4           0  train_splits/dia0_utt4.mp4   \n",
       "\n",
       "                       audio_path  \\\n",
       "0  MELD_Audio/train_dia0_utt0.wav   \n",
       "1  MELD_Audio/train_dia0_utt1.wav   \n",
       "2  MELD_Audio/train_dia0_utt2.wav   \n",
       "3  MELD_Audio/train_dia0_utt3.wav   \n",
       "4  MELD_Audio/train_dia0_utt4.wav   \n",
       "\n",
       "                                       Transcription  duration  split  \\\n",
       "0  also I was the point person on my company's tr...  5.672333  train   \n",
       "1                   You must've had your hands full.  1.501500  train   \n",
       "2                            That I did. That I did.  2.919583  train   \n",
       "3      So let's talk a little bit about your duties.  2.752750  train   \n",
       "4                              My duties? All right.  6.464792  train   \n",
       "\n",
       "   target_stress dataset_origin  \n",
       "0              0           MELD  \n",
       "1              0           MELD  \n",
       "2              0           MELD  \n",
       "3              0           MELD  \n",
       "4              0           MELD  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rutas de origen en Drive:\n",
    "ZIP_PATH_IEMOCAP = '/content/drive/MyDrive/Proyecto_TFG_Data/IEMOCAP_Audio.zip'\n",
    "ZIP_PATH_MELD = '/content/drive/MyDrive/Proyecto_TFG_Data/MELD_Audio.zip'\n",
    "CSV_PATH = '/content/drive/MyDrive/Proyecto_TFG_Data/Multimodal_Stress_Dataset.csv'\n",
    "\n",
    "# Rutas locales en Colab (Entorno local)\n",
    "LOCAL_DATA_ROOT = '/content/data'\n",
    "IEMOCAP_AUDIO_PATH = os.path.join(LOCAL_DATA_ROOT, 'IEMOCAP_Audio')\n",
    "\n",
    "# Rutas de SALIDA en Drive\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_AUDIO'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Copiamos y descomprimimos los archivos ZIP con los datos en el almacenamiento local de Colab (más rápido que trabajar directamente desde Drive):\n",
    "if not os.path.exists(LOCAL_DATA_ROOT):\n",
    "    os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "    \n",
    "    # ------------ MELD --------\n",
    "    if os.path.exists(ZIP_PATH_MELD):\n",
    "        print(f\"Copiando {ZIP_PATH_MELD} a {LOCAL_DATA_ROOT}.\")\n",
    "        shutil.copy(ZIP_PATH_MELD, LOCAL_DATA_ROOT)\n",
    "        with zipfile.ZipFile(os.path.join(LOCAL_DATA_ROOT, 'MELD_Audio.zip'), 'r') as z:\n",
    "            z.extractall(LOCAL_DATA_ROOT)\n",
    "            \n",
    "    # ------------ IEMOCAP ---------\n",
    "    if os.path.exists(ZIP_PATH_IEMOCAP):\n",
    "        print(f\"Copiando {ZIP_PATH_IEMOCAP} a {LOCAL_DATA_ROOT}.\")\n",
    "        shutil.copy(ZIP_PATH_IEMOCAP, LOCAL_DATA_ROOT)\n",
    "        with zipfile.ZipFile(os.path.join(LOCAL_DATA_ROOT, 'IEMOCAP_Audio.zip'), 'r') as z:\n",
    "            z.extractall(LOCAL_DATA_ROOT)\n",
    "else:\n",
    "    print(f\"Los datos ya existen en {LOCAL_DATA_ROOT}.\")\n",
    "\n",
    "# Cargamos del CSV Global\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df_global = pd.read_csv(CSV_PATH)\n",
    "    display(df_global.head()) \n",
    "else:\n",
    "    print(f\"No se encuentra el archivo en {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720b724",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Análisis Previo**\n",
    "\n",
    "A diferencia del vídeo, donde fijamos 32 frames, en audio mantendremos la **longitud variable** para no perder información prosódica importante (silencios, cambios de tono, etc.).\n",
    "\n",
    "* Analizamos la duración de los audios para confirmar la variabilidad de los datos que alimentarán al modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0752270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MELD Audio (seg): Media=3.65, Min=0.08, Max=9.63\n",
      "IEMOCAP Audio (seg): Media=4.13, Min=1.16, Max=12.77\n"
     ]
    }
   ],
   "source": [
    "def analizar_duracion_audio(df, path_meld, path_iemocap, sample_size=50):\n",
    "    \"\"\"\n",
    "    Selecciona un batch aleatorio por cada dataset (MELD e IEMOCAP), de acuerdo al tamaño indicado, y analiza la duración en segundos.\n",
    "    - Devuelve una lista con la media, duración mínima y máxima para cada dataset.\n",
    "    - Nota: Para MELD, si el audio no está extraído, se intentará leer directamente del vídeo asociado.\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for origen in ['MELD', 'IEMOCAP']:\n",
    "        subset = df[df['dataset_origin'] == origen]\n",
    "        rutas = subset['audio_path'].sample(n=sample_size, random_state=42).tolist()\n",
    "\n",
    "        durations = []\n",
    "        for ruta in rutas:\n",
    "            # ---------- MELD ------------\n",
    "            if origen == 'MELD':\n",
    "                # Intentamos buscar el archivo. Si el audio está extraído o usamos el vídeo asociado\n",
    "                # Nota: librosa puede leer audio directamente desde el .mp4 si el .wav no existe\n",
    "                full_path = os.path.join(path_meld, ruta)\n",
    "                if not os.path.exists(full_path): # Si no está el wav, probamos con el video asociado\n",
    "                     video_r = subset[subset['audio_path'] == ruta]['video_path'].values[0]\n",
    "                     full_path = os.path.join(path_meld, video_r)\n",
    "            else:\n",
    "                full_path = os.path.join(path_iemocap, ruta)\n",
    "            \n",
    "            if os.path.exists(full_path):\n",
    "                dur = librosa.get_duration(path=full_path)\n",
    "                durations.append(dur)\n",
    "        \n",
    "        if durations:\n",
    "            resultados.append([np.mean(durations), np.min(durations), np.max(durations)])\n",
    "        else:\n",
    "            resultados.append([0, 0, 0])\n",
    "            \n",
    "    return resultados\n",
    "\n",
    "resultados = analizar_duracion_audio(df_global, LOCAL_DATA_ROOT, IEMOCAP_AUDIO_PATH)\n",
    "print(f\"MELD Audio (seg): Media={resultados[0][0]:.2f}, Min={resultados[0][1]:.2f}, Max={resultados[0][2]:.2f}\")\n",
    "print(f\"IEMOCAP Audio (seg): Media={resultados[1][0]:.2f}, Min={resultados[1][1]:.2f}, Max={resultados[1][2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1336cd5",
   "metadata": {},
   "source": [
    "---\n",
    "## **Wav2Vec 2.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e29905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Se carga inicialmente el modelo WAV2VEC 2.0 (960 horas) ---\n",
    "\n",
    "\n",
    "def extract_wav2vec(audio_path,processor,model):\n",
    "    \"\"\"\n",
    "    Con el modelo Wav2Vec 2.0 (960 horas) cargado previamente, extrae embeddings de audio.\n",
    "    Retorna embeddings de tamaño (Time_Steps, 768).\n",
    "    Sampling Rate obligatorio: 16000 Hz.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargamos audio con obligatoriamente 16kHz:\n",
    "        y, sr = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Aplicamos preprocesamiento (Tokenización + Padding interno del modelo):\n",
    "        inputs = processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = inputs.input_values.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_values)\n",
    "            # outputs.last_hidden_state tiene forma (1, T, 768) -> quitamos el batch con squeeze\n",
    "            hidden_states = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            \n",
    "        return hidden_states\n",
    "    except Exception as e:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a75989",
   "metadata": {},
   "source": [
    "---\n",
    "## **MFCCs + RMS + ZCR (*Hand-crafted* baseline)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4234f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. HAND-CRAFTED (Baseline) ---\n",
    "def extract_handcrafted(audio_path):\n",
    "    \"\"\"\n",
    "    Carga el audio, extrae 13 MFCCs, RMS y ZCR, y los apila en una matriz de características.\n",
    "    Retorna matriz de (Time_Steps, 15).\n",
    "    Compuesta por: 13 MFCCs + 1 RMS + 1 ZCR.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=16000) # sr = 16000 Hz es la frecuencia de muestreo (número de muestra/pasos de tiempo por segundo) (obligatorio para consistencia con Wav2Vec), significa que si el audio no está a esa frecuencia, se re-muestreará automáticamente\n",
    "        hop_length = 512 # Aprox 32ms\n",
    "        \n",
    "        # -----------> 13 MFCCs (Timbre / Tracto Vocal)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=hop_length) #hop_length=512 es que cada 512 muestras/pasos de tiempo se calcula un nuevo vector de características, lo que da una resolución temporal de aproximadamente 32ms (512 muestras a 16kHz)\n",
    "        \n",
    "        # --------->RMS (Energía)\n",
    "        rms = librosa.feature.rms(y=y, hop_length=hop_length)\n",
    "        \n",
    "        # ----------> ZCR (Rugosidad/Ruido)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=y, hop_length=hop_length)\n",
    "        \n",
    "        # Apilamos: (15, T) -> Transponemos a (T, 15)\n",
    "        features = np.vstack([mfcc, rms, zcr]).T\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2eb19",
   "metadata": {},
   "source": [
    "----\n",
    "## ***Feed-Forward*. Extracción de Características**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da215b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]config.json: 0.00B [00:00, ?B/s]tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]Loading weights:   0%|          | 0/210 [00:00<?, ?it/s]\n",
      "Procesando Audio:   0%|          | 47/21219 [00:13<34:24, 10.26it/s]  /usr/local/lib/python3.12/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1280\n",
      "  warnings.warn(\n",
      "Procesando Audio:   0%|          | 90/21219 [00:17<19:04, 18.47it/s]/usr/local/lib/python3.12/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1920\n",
      "  warnings.warn(\n",
      "Procesando Audio:  100%|██████████| 21219/21219 [15:49<00:00, 20.75it/s]\n",
      "Procesando Audio:  100%|██████████| 21219/21219 [15:49<00:00, 17.32it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PROCESAMIENTO ---\n",
    "\n",
    "# Cargamos el modelo Wav2Vec 2.0 (960 horas) una sola vez para usarlo en todo el proceso:\n",
    "# Cargamos también el processor:\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "wav2vec_model.eval()\n",
    "\n",
    "# Definimos un directorio temporal local para ir almacenando los resultados antes de copiarlos a Drive (más rápido trabajar localmente y luego los movemos a Drive):\n",
    "TEMP_LOCAL_DIR = '/content/temp_features_audio'\n",
    "temp_dirs = {\n",
    "    'wav2vec': os.path.join(TEMP_LOCAL_DIR, 'audio_wav2vec'),\n",
    "    'handcrafted': os.path.join(TEMP_LOCAL_DIR, 'audio_handcrafted')\n",
    "}\n",
    "for p in temp_dirs.values(): \n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "for index, row in tqdm(df_global.iterrows(), total=len(df_global), desc=\"Procesando Audio\"):\n",
    "    \n",
    "    # 1. Definimos los nombres de salida\n",
    "    fname = f\"{row['Dialogue_ID']}_{row['Utterance_ID']}.npy\".replace(\"/\", \"_\")\n",
    "    out_w2v = os.path.join(temp_dirs['wav2vec'], fname)\n",
    "    out_hc = os.path.join(temp_dirs['handcrafted'], fname)\n",
    "    \n",
    "    # Chequeo si ya existe (para reanudar si se corta la ejecución):\n",
    "    if os.path.exists(out_w2v) and os.path.exists(out_hc):\n",
    "        continue\n",
    "\n",
    "    # 2. Localizar archivo de entrada\n",
    "    # Nota: Usamos la columna 'audio_path' del CSV, pero si falla, probamos con 'video_path'\n",
    "    # ya que librosa puede extraer el audio del vídeo mp4 directamente.\n",
    "    \n",
    "    if row['dataset_origin'] == 'MELD':\n",
    "        # MELD suele tener estructura compleja.\n",
    "        # Intento 1: Ruta exacta del audio csv\n",
    "        audio_path = os.path.join(LOCAL_DATA_ROOT, row['audio_path'])\n",
    "        if not os.path.exists(audio_path):\n",
    "            # Intento 2: Ruta del vídeo (librosa extraerá el audio)\n",
    "            audio_path = os.path.join(LOCAL_DATA_ROOT, row['video_path'])\n",
    "    else:\n",
    "        # IEMOCAP\n",
    "        audio_path = os.path.join(LOCAL_DATA_ROOT, row['audio_path'])\n",
    "        if not os.path.exists(audio_path):\n",
    "             audio_path = os.path.join(LOCAL_DATA_ROOT, row['video_path'])\n",
    "    \n",
    "    # 3. Procesar si existe el archivo\n",
    "    if os.path.exists(audio_path):\n",
    "        # Wav2Vec\n",
    "        emb_w2v = extract_wav2vec(audio_path, processor, wav2vec_model)\n",
    "        # Handcrafted\n",
    "        emb_hc = extract_handcrafted(audio_path)\n",
    "        \n",
    "        if emb_w2v is not None and emb_hc is not None:\n",
    "            np.save(out_w2v, emb_w2v)\n",
    "            np.save(out_hc, emb_hc)\n",
    "\n",
    "# --- SUBIDA A DRIVE ---\n",
    "# Comprimimos la carpeta temporal con los resultados:\n",
    "shutil.make_archive(\"/content/features_audio_COMPLETO\", 'zip', TEMP_LOCAL_DIR)\n",
    "shutil.copy(\"/content/features_audio_COMPLETO.zip\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f923ad",
   "metadata": {},
   "source": [
    "### **Sanity Check**\n",
    "\n",
    "Verificamos finalmente para asegurar que tenemos todos los *embeddings* cargados correctamente (conteo), además de mostrar un ejemplo verificando ese tamaño variable ya mencionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total esperado: 21219\n",
      "Wav2Vec generados: 21219\n",
      "Handcrafted generados: 21219\n",
      "\n",
      "Ejemplo (308_train_dia308_utt5.npy):\n",
      "Wav2Vec Shape: (112, 768) \n",
      "Handcrafted Shape: (71, 15)\n"
     ]
    }
   ],
   "source": [
    "# Listar archivos generados en local\n",
    "files_w2v = os.listdir(temp_dirs['wav2vec'])\n",
    "files_hc = os.listdir(temp_dirs['handcrafted'])\n",
    "\n",
    "print(f\"Total esperado: {len(df_global)}\")\n",
    "print(f\"Wav2Vec generados: {len(files_w2v)}\")\n",
    "print(f\"Handcrafted generados: {len(files_hc)}\")\n",
    "\n",
    "# Verificar variabilidad de dimensiones (Ejemplo aleatorio)\n",
    "if len(files_w2v) > 0:\n",
    "    sample = random.choice(files_w2v)\n",
    "    \n",
    "    data_w2v = np.load(os.path.join(temp_dirs['wav2vec'], sample))\n",
    "    data_hc = np.load(os.path.join(temp_dirs['handcrafted'], sample))\n",
    "    \n",
    "    print(f\"\\nEjemplo ({sample}):\")\n",
    "    print(f\"Wav2Vec Shape: {data_w2v.shape}\")\n",
    "    print(f\"Handcrafted Shape: {data_hc.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

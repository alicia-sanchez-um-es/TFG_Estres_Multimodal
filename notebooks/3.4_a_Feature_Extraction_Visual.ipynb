{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ba1faa",
   "metadata": {},
   "source": [
    "## **Extracción de Características Visuales (Visual Embeddings)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669ea89",
   "metadata": {},
   "source": [
    "En este notebook abordamos la fase de **Feature Extraction** (Extracción de Características) para la modalidad de vídeo. El objetivo principal es transformar la información \"cruda\" de los vídeos (píxeles) en representaciones compactas de significado (**Embeddings**), que posteriormente alimentarán a los modelos de Deep Learning en la fase de experimentación.\n",
    "\n",
    "Este paso es esencial ya que los vídeos originales de ambos *datasets* contienen una gran cantidad de información redundante (miles de píxeles por frame, repetidos 30 veces por segundo, por ejemplo). Entrenar nuestros modelos directamente con los píxeles en crudo es computacionalmente inviable y puede derivar al *overfitting* de nuestros. Por tanto, aplicamos una estrategia de **Transfer Learning** donde utilizaremos redes neuronales convolucionales (CNNs) y transformers ya pre-entrenadas en millones de imágenes (ImageNet) para que actúen como \"extractores de características\". Estas redes ya *saben ver* (detectan bordes, formas, texturas y patrones faciales complejos), por lo que podemos usar sus capas internas para resumir cada frame/s del vídeo en un vector numérico de alta densidad.\n",
    "\n",
    "Para ello, siguiendo la metodología, extraemos las características de **tres arquitecturas diferentes seleccionadas** para poder comparar su rendimiento:\n",
    "\n",
    "1. **ResNet50 (Residual Networks):** Estándar en la industria de la Visión Artificial. Utiliza conexiones residuales para permitir redes muy profundas. Genera vectores de **2048 dimensiones** por defecto.\n",
    "\n",
    "2. **EfficientNet-B0:** Arquitectura diseñada para ser extremadamente eficiente en recursos computacionales sin perder precisión. Genera vectores de **1280 dimensiones** por defecto, lo que ahorrará espacio y tiempo de cómputo.\n",
    "\n",
    "3. **ViT-B/16 (Vision Transformer):** Arquitectura base basada en Transformer que ha demostrado excelente rendimiento en tareas de visión artificial. Procesa las imágenes como secuencias de parches y genera vectores de **768 dimensiones** por defecto. Se ha seleccionado la versión **Base** de **ViT** en lugar del **Large** (1024) ya que, como ya hemos justificado, los vídeos de MELD e IEMOCAP son vídeos cortos en su mayoría, por tanto una dimensión de 1024 introduciría demasiada redundancia, lo que provocaría que la red hiciera *overfitting*.\n",
    "\n",
    "\n",
    "**NOTA:** Dado que la emoción es dinámica, no podemos usar solo una imagen, pero tampoco el vídeo entero. Se lleva a cabo a continuación un **análisis previo** para la decisión del número de *frames* que extraeremos del vídeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Carga previa de todas las librerías y paquetes necesarios\n",
    "import os\n",
    "import cv2 # Para leer los vídeos\n",
    "import numpy as np  \n",
    "import torch \n",
    "import pandas as pd\n",
    "import torch.nn as nn  # Para modificar las capas finales de los modelos (eliminarlas) \n",
    "from torchvision import models, transforms \n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # Para barra de progreso\n",
    "import shutil\n",
    "from google.colab import drive\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "# Configuración de GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028e7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Se monta Drive para acceder a los datos y guardar los resultados:\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cb603",
   "metadata": {},
   "source": [
    "Definimos las rutas desde las cuales se cargan los archivos comprimidos con los *embeddings* desde **Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copiando /content/drive/MyDrive/Proyecto_TFG_Data/MELD_CLIPS.zip a /content/data.\n",
      "Copiando /content/drive/MyDrive/Proyecto_TFG_Data/IEMOCAP_CLIPS.zip a /content/data.\n"
     ]
    }
   ],
   "source": [
    "LOCAL_DATA_ROOT = '/content/data' # Directorio local en Colab para almacenar los datos\n",
    "\n",
    "# ----- RUTAS ORIGEN (DRIVE) ----\n",
    "# Archivos .tar.gz con los embeddings ya extraídos\n",
    "TAR_PATH_RESNET = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_VISUAL/features_resnet.tar.gz'  \n",
    "TAR_PATH_EFFICIENTNET = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_VISUAL/features_efficientnet.tar.gz'\n",
    "TAR_PATH_VIT = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_VISUAL/features_vit.tar.gz'\n",
    "\n",
    "MELD_CLIPS_PATH = os.path.join(LOCAL_DATA_ROOT, 'MELD_CLIPS')  # Directorio donde se descomprimirán los vídeos de MELD\n",
    "IEMOCAP_CLIPS_PATH = os.path.join(LOCAL_DATA_ROOT, 'IEMOCAP_CLIPS')  # Directorio donde se descomprimirán los vídeos de IEMOCAP\n",
    "\n",
    "# ---------------- RUTAS DESTINO (LOCAL COLAB) ----------------\n",
    "# Rutas donde se guardarán los embeddings extraídos por cada modelo (después de descomprimir los .tar.gz)\n",
    "OUTPUT_DIR_RESNET = os.path.join(LOCAL_DATA_ROOT, 'EMBEDDINGS_VISUAL_RESNET')\n",
    "OUTPUT_DIR_EFFICIENTNET = os.path.join(LOCAL_DATA_ROOT, 'EMBEDDINGS_VISUAL_EFFICIENTNET')\n",
    "OUTPUT_DIR_VIT = os.path.join(LOCAL_DATA_ROOT, 'EMBEDDINGS_VISUAL_ViT')\n",
    "\n",
    "MELD_CLIPS_PATH = os.path.join(LOCAL_DATA_ROOT, 'MELD_CLIPS')  \n",
    "IEMOCAP_CLIPS_PATH = os.path.join(LOCAL_DATA_ROOT, 'IEMOCAP_CLIPS')\n",
    "\n",
    "# CSV:\n",
    "DATA_ROOT_CSV = '/content/drive/MyDrive/Proyecto_TFG_Data/Multimodal_Stress_Dataset.csv' \n",
    "\n",
    "# ------------ EXTRACCIÓN DE VÍDEOS (.zip) ------------\n",
    "ZIP_PATH_MELD_CLIPS_DRIVE = '/content/drive/MyDrive/Proyecto_TFG_Data/MELD_CLIPS.zip'  \n",
    "ZIP_PATH_IEMOCAP_CLIPS_DRIVE = '/content/drive/MyDrive/Proyecto_TFG_Data/IEMOCAP_CLIPS.zip'\n",
    "\n",
    "zip_files = {\n",
    "    ZIP_PATH_MELD_CLIPS_DRIVE: MELD_CLIPS_PATH,\n",
    "    ZIP_PATH_IEMOCAP_CLIPS_DRIVE: IEMOCAP_CLIPS_PATH\n",
    "}\n",
    "\n",
    "for zip_drive_path, local_path in zip_files.items():\n",
    "    if not os.path.exists(local_path) or len(os.listdir(local_path)) == 0:\n",
    "        print(f\"Copiando {zip_drive_path} a {LOCAL_DATA_ROOT}.\")\n",
    "        os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "        shutil.copy(zip_drive_path, LOCAL_DATA_ROOT)\n",
    "        with zipfile.ZipFile(os.path.join(LOCAL_DATA_ROOT, os.path.basename(zip_drive_path)), 'r') as zip_ref:\n",
    "            zip_ref.extractall(LOCAL_DATA_ROOT)\n",
    "    else:\n",
    "        print(f\"La estructura de directorios de {os.path.basename(zip_drive_path)} ya existe en {LOCAL_DATA_ROOT}. No se realizará la copia ni descompresión.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d054e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2e186fa4-8f88-4ad1-9561-ce1fb6df2e13\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>video_path</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "      <th>target_stress</th>\n",
       "      <th>dataset_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_dia0_utt0</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt0.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt0.wav</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>5.672333</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_dia0_utt1</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt1.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt1.wav</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>1.501500</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_dia0_utt2</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt2.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt2.wav</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>2.919583</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_dia0_utt3</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt3.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt3.wav</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>2.752750</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_dia0_utt4</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt4.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt4.wav</td>\n",
       "      <td>My duties? All right.</td>\n",
       "      <td>6.464792</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e186fa4-8f88-4ad1-9561-ce1fb6df2e13')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2e186fa4-8f88-4ad1-9561-ce1fb6df2e13 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2e186fa4-8f88-4ad1-9561-ce1fb6df2e13');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Utterance_ID Dialogue_ID                  video_path  \\\n",
       "0  train_dia0_utt0           0  train_splits/dia0_utt0.mp4   \n",
       "1  train_dia0_utt1           0  train_splits/dia0_utt1.mp4   \n",
       "2  train_dia0_utt2           0  train_splits/dia0_utt2.mp4   \n",
       "3  train_dia0_utt3           0  train_splits/dia0_utt3.mp4   \n",
       "4  train_dia0_utt4           0  train_splits/dia0_utt4.mp4   \n",
       "\n",
       "                       audio_path  \\\n",
       "0  MELD_Audio/train_dia0_utt0.wav   \n",
       "1  MELD_Audio/train_dia0_utt1.wav   \n",
       "2  MELD_Audio/train_dia0_utt2.wav   \n",
       "3  MELD_Audio/train_dia0_utt3.wav   \n",
       "4  MELD_Audio/train_dia0_utt4.wav   \n",
       "\n",
       "                                       Transcription  duration  split  \\\n",
       "0  also I was the point person on my company's tr...  5.672333  train   \n",
       "1                   You must've had your hands full.  1.501500  train   \n",
       "2                            That I did. That I did.  2.919583  train   \n",
       "3      So let's talk a little bit about your duties.  2.752750  train   \n",
       "4                              My duties? All right.  6.464792  train   \n",
       "\n",
       "   target_stress dataset_origin  \n",
       "0              0           MELD  \n",
       "1              0           MELD  \n",
       "2              0           MELD  \n",
       "3              0           MELD  \n",
       "4              0           MELD  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carga del dataset global\n",
    "\n",
    "file_path = DATA_ROOT_CSV\n",
    "if os.path.exists(file_path):\n",
    "    df_global = pd.read_csv(file_path)\n",
    "    display(df_global.head()) \n",
    "else:\n",
    "    print(f\"No se encuentra el archivo en {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dfb1d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951f807",
   "metadata": {},
   "source": [
    "#### **ESTRUCTURA NOTEBOOK Y EJECUCIÓN**:\n",
    "\n",
    "Debido a la alta carga computacional que supone el procesado de los *clips* de vídeo de ambos datasets para la extracción de los embeddings, y a esto se le suman los **3 diferentes codificadores** que utilizaremos para ello, se ha optado por realizar la carga y ejecución en el Servidor NVIDIA DGX de **CyberDataLab**, al cual he tenido acceso para realizar el TFG. \n",
    "\n",
    "A continuación, se presenta el análisis inicial llevado a cabo, previo a la extracción de características visuales, y, con un fin demostrativo, se indica y explica el código de inicialización de cada modelo a utilizar (**ResNet50**, **EfficientNet-B0** y **ViT-B/16**), así como el *script* final lanzado en el servidor para la ejecución. \n",
    "\n",
    "Finalmente, se cargan los *embeddings* resultantes (empaquetados y almacenados en **Drive**) en el servidor local de **Colab** para realizar una auditoría final de validación (**sanity check**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a717c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Análisis Previo**\n",
    "\n",
    "* Identificamos el número de *frames* promedio en una muestra aleatoria para justificar la decisión de cuántos *frames* se extraerán del video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90812636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados Frames en MELD: \n",
      "Media de frames por vídeo: 87.42\n",
      "Mínimo: 2.00, Máximo: 231.00\n",
      "\n",
      "Resultados Frames en IEMOCAP: \n",
      "Media de frames por vídeo: 123.14\n",
      "Mínimo: 34.00, Máximo: 382.00\n",
      "\n",
      "Con 32 frames, capturamos aprox el 26.0% de la información para IEMOCAP y el 36.6% de la información para MELD\n"
     ]
    }
   ],
   "source": [
    "# --- ANÁLISIS PREVIO DE FRAMES ---\n",
    "\n",
    "def analizar_densidad_frames(df,path_clips_meld, path_clips_iemocap, sample_size=50):\n",
    "    \"\"\"\n",
    "    Selecciona un batch aleatorio POR CADA DATASET (MELD e IEMOCAP) de vídeos de acuerdo a la longitud indicada (sample_size).\n",
    "    Devuelve:\n",
    "    - resultados: Lista que incluye los resultados de la media (mean), mínimo (min) y máximo (max) de los vídeos del bacth por cada dataset (MELD e IEMOCAP)\n",
    "    \"\"\"\n",
    "    resultados = []  # Aquí almacenamos los resultados obtenidos a devolver\n",
    "    for origen in ['MELD','IEMOCAP'] :\n",
    "        subset = df[df['dataset_origin'] == origen]  # Seleccionamos vídeos de dicho dataset únicamente\n",
    "        # Cogemos rutas aleatorias para los vídeos de dicho subset son random_state fijo para reproducibilidad\n",
    "        rutas = subset['video_path'].sample(n=sample_size, random_state=42).tolist()\n",
    "    \n",
    "        frame_counts = []\n",
    "        for ruta in rutas:\n",
    "            # Con OpenCV, abrimos cada vídeo y contamos el número de frames que tiene. Si el vídeo no se puede abrir o no tiene frames, lo ignoramos.\n",
    "            # ----- MELD -----------\n",
    "            if df[df['video_path'] == ruta]['dataset_origin'].values[0] == 'MELD':\n",
    "                cap = cv2.VideoCapture(os.path.join(path_clips_meld, ruta))\n",
    "            else:\n",
    "                # ----- IEMOCAP ------\n",
    "                cap = cv2.VideoCapture(os.path.join(path_clips_iemocap, ruta))\n",
    "            if cap.isOpened():\n",
    "                frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if frames > 0: frame_counts.append(frames)\n",
    "            cap.release()\n",
    "    \n",
    "        if frame_counts:\n",
    "            media = np.mean(frame_counts)\n",
    "            max = np.max(frame_counts)\n",
    "            min = np.min(frame_counts)\n",
    "            resultados.append([media,min,max])\n",
    "        else:\n",
    "            print(\"No se pudieron leer los vídeos.\")\n",
    "            resultados.append([0,0,0])\n",
    "    return resultados\n",
    "\n",
    "\n",
    "resultados_datasets = analizar_densidad_frames(df_global, MELD_CLIPS_PATH, LOCAL_DATA_ROOT)\n",
    "print(f\"Resultados Frames en MELD: \\nMedia de frames por vídeo: {resultados_datasets[0][0]:.2f}\\nMínimo: {resultados_datasets[0][1]:.2f}, Máximo: {resultados_datasets[0][2]:.2f}\")\n",
    "print(f\"\\nResultados Frames en IEMOCAP: \\nMedia de frames por vídeo: {resultados_datasets[1][0]:.2f}\\nMínimo: {resultados_datasets[1][1]:.2f}, Máximo: {resultados_datasets[1][2]:.2f}\")\n",
    "print(f\"\\nCon 32 frames, capturamos aprox el {32/resultados_datasets[1][0]*100:.1f}% de la información para IEMOCAP y el {32/resultados_datasets[0][0]*100:.1f}% de la información para MELD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67b8c6",
   "metadata": {},
   "source": [
    "Tras este análisis realizado de densidad temporal, se ha decidido establecer la resolución de muestreo a **32 frames** por vídeo. Esto permite capturar en **IEMOCAP** aproximadamente el **26%** de la información en **IEMOCAP** y un **36.6%** en **MELD** sin tener que explotar la **RAM**. Además, esta decisión permite capturar una frecuenica de muestreo de, aproximadamente, **6-8 FPS** (dependiendo del dataset), suficiente para registrar micro-expresiones faciales que ocurren en intervalos inferiores a 200 ms.\n",
    "\n",
    "**CONCLUSIÓN**: Con **32 frames** (`num_frames = 32`), el tensor resultante tendrá dimensiones `(32,dim_model)` (dependiendo del modelo), garantizando un mayor balance entre ganularidad temporal y viabilidad de memoria para el entrenamiento de las redes recurrentes (**LSTM**). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1931a",
   "metadata": {},
   "source": [
    "Definimos a continuación una función que nos extraiga los *frames* con el número indicado (en nuestro caso, **32**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=32):\n",
    "    \"\"\"\n",
    "    Abre un vídeo y extrae 'num_frames' imágenes distribuidas uniformemente.\n",
    "    Devuelve: Una lista de imágenes en formato PIL.\n",
    "    \"\"\"\n",
    "    # Abrimos el vídeo con OpenCV (en formato BGR, que es el formato nativo de OpenCV y evita gasto de CPU en convertir a RGB si no es necesario)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Verificamos si el vídeo abre\n",
    "    if not cap.isOpened():\n",
    "        return [] # Si no se puede abrir el vídeo, devolvemos una lista vacía\n",
    "\n",
    "    # Leemos todos los frames del vídeo y los almacenamos en una lista:\n",
    "    all_frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # Si no se pueden leer más frames, salimos del bucle, ya que hemos leído todos los frames disponibles\n",
    "        all_frames.append(frame) # Guardamos BGR \n",
    "    \n",
    "    cap.release() # Cerramos el vídeo para liberar recursos\n",
    "\n",
    "    total_frames = len(all_frames)\n",
    "    if total_frames == 0:\n",
    "        return []  # Por seguridad, si el vídeo no tiene frames, devolvemos una lista vacía\n",
    "\n",
    "    # Calculamos qué índices coger, con np.linspace creamos una secuencia espaciada uniformemente (ej: 0, 5, 10...)\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    # Esto asegura que aunque el vídeo tenga menos de 'num_frames', no se generarán índices fuera del rango, y se repetirá el último frame según sea necesario\n",
    "    \n",
    "    # Solo convertimos a RGB y PIL los 32 frames seleccionados, para evitar gasto de CPU en convertir a RGB los frames que no vamos a usar:\n",
    "    selected_frames = []\n",
    "    for i in indices:\n",
    "        frame_bgr = all_frames[i]\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        selected_frames.append(Image.fromarray(frame_rgb))\n",
    "    \n",
    "    # Padding de seguridad si el video era muy corto (menos de 32 frames), repetimos el último frame hasta completar los 32:\n",
    "    while len(selected_frames) < num_frames:\n",
    "        selected_frames.append(selected_frames[-1])\n",
    "        \n",
    "    return selected_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852068f",
   "metadata": {},
   "source": [
    "---\n",
    "## **ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea887446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_extractor():\n",
    "    \"\"\"\n",
    "    Carga ResNet50 pre-entrenada y elimina la capa de clasificación (FC).\n",
    "    Devuelve:\n",
    "    - model: El modelo extractor listo para usar.\n",
    "    - output_dim: 2048 (Número de características por frame)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargamos los pesos entrenados en ImageNet. Esto asegura que la red sepa ya reconocer formas, luces y texturas (sesgo inductivo de la CNN):\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    \n",
    "    # ----- ELIMINAMOS ÚLTIMA CAPA DE CLASIFICACIÓN -----\n",
    "    # La arquitectura ResNet original termina en una capa 'fc' (Linear) que clasifica en 1000 clases.\n",
    "    # Nosotros queremos el vector justo antes de esa clasificación\n",
    "    modules = list(model.children())[:-1] # Con list(model.children())[:-1] cogemos todas las capas menos la última\n",
    "    model = nn.Sequential(*modules) # Creamos un nuevo modelo con todas las capas excepto la última\n",
    "    \n",
    "    # Congelamos los pesos (Freeze), ya que no queremos entrenar la CNN, solo usarla directamente:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval() # Ponemos en modo evaluación \n",
    "    \n",
    "    return model, 2048  # ResNet50 siempre devuelve vectores de tamaño 2048\n",
    "\n",
    "# Definimos también las transformaciones que ResNet (al igual que EfficientNet) necesita para funcionar:\n",
    "# Son reglas estrictas de ImageNet: normalización con media/std específicos\n",
    "preprocess = transforms.Compose([ \n",
    "    transforms.Resize(256), # Redimensionamos a 256x256\n",
    "    transforms.CenterCrop(224), # Recortamos al centro para quedarnos con 224x224, que es el tamaño que ResNet y EfficientNet esperan\n",
    "    transforms.ToTensor(), # Convertimos a tensor (esto también escala los píxeles a [0,1])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalización específica de ImageNet\n",
    "])\n",
    "\n",
    "# Con esto, ya tenemos lista la configuración y preprocesamiento de los frames para ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d2d01",
   "metadata": {},
   "source": [
    "-------\n",
    "## **EfficientNet-B0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acfad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnet_extractor():\n",
    "    \"\"\"\n",
    "    Carga EfficientNet-B0 y adapta para extracción de características.\n",
    "    Devuelve:\n",
    "    - model: El extractor.\n",
    "    - output_dim: 1280\n",
    "    \"\"\"\n",
    "    \n",
    "    #  Cargamos los pesos\n",
    "    weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = models.efficientnet_b0(weights=weights)\n",
    "    \n",
    "    # -----ELIMINAMOS ÚLTIMA CAPA DE CLASIFICACIÓN -----\n",
    "    # En EfficientNet, la capa de clasificación se llama 'classifier'\n",
    "    # Únicamente queremos lo que se obtiene de las 'features' y pasa por el 'avgpool':\n",
    "    model = nn.Sequential(\n",
    "        model.features,  #Seleecionamos las features \n",
    "        model.avgpool  # y avgpool\n",
    "    )\n",
    "    \n",
    "    # Congelamos los pesos:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, 1280  # EfficientNet-B0 devuelve vectores de 1280\n",
    "\n",
    "# Se definen las mismas transformaciones que para ResNet (preprocess), ya que es el estándar en ImageNet.\n",
    "# Con esto ya tenemos configurado EfficientNet-B0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcd8cb",
   "metadata": {},
   "source": [
    "-------\n",
    "## **ViT (Vision Transformer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_extractor():\n",
    "    \"\"\"\n",
    "    Carga ViT-B/16 pre-entrenado y adapta para extracción de características.\n",
    "    Devuelve:\n",
    "    - model: El extractor.\n",
    "    - output_dim: 768\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargamos los pesos pre-entrenados de ViT-B/16\n",
    "    weights = models.ViT_B_16_Weights.DEFAULT\n",
    "    model = models.vit_b_16(weights=weights)\n",
    "    # -----ELIMINAMOS ÚLTIMA CAPA DE CLASIFICACIÓN -----\n",
    "    # En ViT, la capa de clasificación se llama 'head'. Para extraer las características directamente, \n",
    "    # podemos reemplazar esta capa por una identidad, lo que hará que el modelo devuelva directamente el vector de características (el CLS token)\n",
    "    model.head = nn.Identity()\n",
    "    \n",
    "    # Congelamos los pesos:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, 768  # ViT-B/16 devuelve vectores de 768\n",
    "\n",
    "# Las transformaciones son las mismas que para ResNet y EfficientNet\n",
    "# Con esto ya tenemos configurado ViT-B/16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570eeb35",
   "metadata": {},
   "source": [
    "----\n",
    "### ***Feed-Forward*. Extracción de características.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb122d7",
   "metadata": {},
   "source": [
    "Una vez ya tenemos definido y cargados los modelos, extraemos los embeddings de cada uno de los vídeos (y los almacenamos en los directorios creados):\n",
    "\n",
    "**NOTA**: Las anteriores celdas de código muestran, a modo de ejemplo, la carga de cada uno de los modelos de forma más detallada, con motivo principalmente demostrativo. A continuación, se presenta la celda de código exacta del script `.py` ejecutado en el Servidor DGX. Los *embeddings* se obtienen inicialmente en este servidor, y luego se empaquetan (`.tar.gz`) y se envían para almacernalos en **Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e85825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "#  BLOQUE DE EJECUCIÓN EN SERVIDOR DGX \n",
    "# ===========================================================================\n",
    "# Debido a la carga computacional de procesar el dataset completo con tres\n",
    "# arquitecturas (especialmente Vision Transformers), este proceso se ha ejecutado\n",
    "# en el servidor NVIDIA DGX Spark (cyberdatalab.um.es) mediante un script .py optimizado.\n",
    "#\n",
    "# A continuación se muestra el código exacto utilizado en dicho script para la\n",
    "# generación de los embeddings. Esta celda se mantiene como bloque demostrativo.\n",
    "# ======================================================================================\n",
    "\n",
    "# Carga previa de todas las librerías y paquetes necesarios para el script:\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1. Definición del Dataset para Carga Paralela (Multiprocessing)\n",
    "# ------------------------------------------------------------------------\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df, root_meld, root_iemocap, transform=None):\n",
    "        self.df = df\n",
    "        self.root_meld = root_meld\n",
    "        self.root_iemocap = root_iemocap\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Selección de ruta según el origen del vídeo\n",
    "        if row['dataset_origin'] == 'MELD':\n",
    "            video_path = os.path.join(self.root_meld, row['video_path'])\n",
    "        else:\n",
    "            video_path = os.path.join(self.root_iemocap, row['video_path'])\n",
    "        \n",
    "        # Extracción de 32 frames uniformes del vídeo\n",
    "        frames = self.extract_frames(video_path)\n",
    "        if self.transform and len(frames) > 0:\n",
    "            frames = torch.stack([self.transform(f) for f in frames])\n",
    "        \n",
    "        # Retornamos el tensor de frames y el nombre formateado para el archivo .npy\n",
    "        file_name = f\"{row['Dialogue_ID']}_{row['Utterance_ID']}.npy\".replace(\"/\", \"_\")\n",
    "        return frames, file_name\n",
    "\n",
    "    def extract_frames(self, video_path, num_frames=32):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        all_frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            all_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        cap.release()\n",
    "        \n",
    "        if not all_frames: return []\n",
    "        indices = np.linspace(0, len(all_frames) - 1, num_frames, dtype=int)\n",
    "        return [Image.fromarray(all_frames[i]) for i in indices]\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Lógica Principal de Extracción\n",
    "# -------------------------------------------------------------------------\n",
    "def main_extraction(model_name, num_workers=8):\n",
    "    \"\"\"\n",
    "    Función principal de extracción diseñada para ejecución paralela en HPC.\n",
    "    Args:\n",
    "    - model_name (str): Nombre del modelo a usar ('resnet', 'efficientnet' o 'vit').\n",
    "    - num_workers (int): Número de procesos paralelos para DataLoader.\n",
    "    Devuelve:\n",
    "    - Guarda los embeddings extraídos en archivos .npy en el directorio correspondiente.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dir = f\"./features_{model_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Configuración del Extractor según Arquitectura\n",
    "    if model_name == 'resnet':\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        model = nn.Sequential(*list(m.children())[:-1])  # Eliminamos la capa de clasificación\n",
    "    elif model_name == 'efficientnet':\n",
    "        m = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        model = nn.Sequential(m.features, m.avgpool) # Solo queremos las features y avgpool, no la clasificación\n",
    "    elif model_name == 'vit':\n",
    "        # Instanciamos el modelo base\n",
    "        model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "        # Reemplazamos la caabeza (la capa que clasifica en 1000 clases) por una identidad para obtener las features directamente\n",
    "        # Esto hace que el modelo devuelva directamente el vector de características (el CLS token)\n",
    "        model.head = nn.Identity()\n",
    "    \n",
    "    for p in model.parameters(): \n",
    "        p.requires_grad = False\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # 2. Pipeline de Preprocesamiento e Inferencia\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # 3. Ejecución Paralela (DataLoader con Workers)\n",
    "    df = pd.read_csv(\"Multimodal_Stress_Dataset.csv\")\n",
    "    dataset = VideoDataset(df, \"./data/MELD_CLIPS\", \"./data\", transform=preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, num_workers=num_workers)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, name in tqdm(dataloader, desc=f\"Extrayendo {model_name}\"):\n",
    "            if frames.shape[1] == 0: \n",
    "                continue\n",
    "            \n",
    "            frames = frames.squeeze(0).to(device) # Shape: (32, 3, 224, 224)\n",
    "            embeddings = model(frames)\n",
    "            # Embeddings shape tras pasar por los modelos:\n",
    "            # ResNet/EfficientNet -> (32, Dim, 1, 1) -> Necesitamos aplicar flatten\n",
    "            # ViT (con Wrapper) -> (32, 768)  -> Ya está plano\n",
    "            \n",
    "            if model_name != 'vit':\n",
    "                embeddings = embeddings.flatten(start_dim=1)\n",
    "            \n",
    "            np.save(os.path.join(output_dir, name[0]), embeddings.cpu().numpy())\n",
    "    \n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PUNTO DE ENTRADA DEL SCRIPT (Main)\n",
    "# -------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # En el script real (.py), el modelo se recibe como argumento de consola, al igual que el número de workers para la ejecución paralela:\n",
    "    # >>> python feature_extraction.py --model vit --workers 16\n",
    "    \n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--model', type=str, required=True)\n",
    "    # parser.add_argument('--workers', type=int, default=8)\n",
    "    # args = parser.parse_args()\n",
    "    # main_extraction(args.model, num_workers=args.workers)\n",
    "\n",
    "    pass # En este bloque no se ejecuta nada, ya que la extracción se ha realizado en el servidor DGX mediante el script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fc1b6",
   "metadata": {},
   "source": [
    "#### **EXTRACCIÓN EMBEDDINGS RESULTANTES DESDE DRIVE (`.tar.gz`)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425c19d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descomprimiendo features_resnet.tar.gz en /content/data/EMBEDDINGS_VISUAL_RESNET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2851875848.py:12: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar_ref.extractall(path=extract_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descomprimiendo features_efficientnet.tar.gz en /content/data/EMBEDDINGS_VISUAL_EFFICIENTNET...\n",
      "Descomprimiendo features_vit.tar.gz en /content/data/EMBEDDINGS_VISUAL_ViT...\n"
     ]
    }
   ],
   "source": [
    "tar_files = {\n",
    "    TAR_PATH_RESNET: OUTPUT_DIR_RESNET,\n",
    "    TAR_PATH_EFFICIENTNET: OUTPUT_DIR_EFFICIENTNET,\n",
    "    TAR_PATH_VIT: OUTPUT_DIR_VIT\n",
    "}\n",
    "\n",
    "for tar_path, extract_dir in tar_files.items():\n",
    "    if not os.path.exists(extract_dir) or len(os.listdir(extract_dir)) == 0:\n",
    "        print(f\"Descomprimiendo {os.path.basename(tar_path)} en {extract_dir}...\")\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar_ref: #'r:gz': lectura de archivo gzip comprimido\n",
    "            tar_ref.extractall(path=extract_dir)\n",
    "    else:\n",
    "        print(f\"Los embeddings ya existen en local: {extract_dir}. \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fcaf0",
   "metadata": {},
   "source": [
    "### **Sanity Check**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56e33c",
   "metadata": {},
   "source": [
    "Realizamos a continuación una verficación rápida de que todos los *embeddings* se han obtenido correctamente (en total y por cada corpus/partición), además de que se han guardado correctamente en el cloud (**Google Drive**).\n",
    "\n",
    "Este *Sanity Check* realiza lo siguiente:\n",
    "* Comprueba el número de archivos guardados (debe ser igual a los vídeos indicados en el CSV).\n",
    "* Muestra un ejemplo de archivo guardado por cada modelo (**ResNet**, **EfficientNet** y **ViT**) para verificar que se han guardado correctamente.\n",
    "* Se muestran mensajes de error claros en caso de un guardado incorrecto, indicando el número de archivos **esperados** en relación a los **encontrados** (obtenidos).\n",
    "* Muestra el número de embeddings obtenidos por cada *dataset* (**MELD** e **IEMOCAP**), además de por cada *split*/partición (`train`, `test`, `dev`) para verificar que se han guardado correctamente los archivos de *embeddings* de ambos *datasets* y de cada *split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDACIÓN DE EMBEDDINGS GUARDADOS:\n",
      "Vídeos esperados (CSV): 21219\n",
      "Embeddings ResNet guardados: 21219\n",
      "Embeddings EfficientNet guardados: 21219\n",
      "Embeddings ViT guardados: 21219\n",
      "\n",
      "VALIDACIÓN POR DATASET:\n",
      "\n",
      "MELD:\n",
      "Vídeos esperados: 13704\n",
      "ResNet embeddings: 13704 (100.0%)\n",
      "EfficientNet embeddings: 13704 (100.0%)\n",
      "ViT embeddings: 13704 (100.0%)\n",
      "\n",
      "IEMOCAP:\n",
      "Vídeos esperados: 7515\n",
      "ResNet embeddings: 7515 (100.0%)\n",
      "EfficientNet embeddings: 7515 (100.0%)\n",
      "ViT embeddings: 7515 (100.0%)\n",
      "\n",
      "VALIDACIÓN POR SPLIT/PARTICIÓN:\n",
      "\n",
      "TRAIN:\n",
      "Vídeos esperados: 14318\n",
      "ResNet embeddings: 14318 (100.0%)\n",
      "EfficientNet embeddings: 14318 (100.0%)\n",
      "ViT embeddings: 14318 (100.0%)\n",
      "\n",
      "DEV:\n",
      "Vídeos esperados: 2644\n",
      "ResNet embeddings: 2644 (100.0%)\n",
      "EfficientNet embeddings: 2644 (100.0%)\n",
      "ViT embeddings: 2644 (100.0%)\n",
      "\n",
      "TEST:\n",
      "Vídeos esperados: 4257\n",
      "ResNet embeddings: 4257 (100.0%)\n",
      "EfficientNet embeddings: 4257 (100.0%)\n",
      "ViT embeddings: 4257 (100.0%)\n",
      "\n",
      "EJEMPLOS DE EMBEDDINGS GUARDADOS:\n",
      "\n",
      "ResNet ejemplo: 12_dev_dia12_utt2.npy\n",
      "Shape: (32, 2048)\n",
      "Tamaño: 256.0 KB\n",
      "Tipo: float32\n",
      "\n",
      "EfficientNet ejemplo: 12_dev_dia12_utt2.npy\n",
      "Shape: (32, 1280)\n",
      "Tamaño: 160.0 KB\n",
      "Tipo: float32\n",
      "\n",
      "ViT ejemplo: 12_dev_dia12_utt2.npy\n",
      "Shape: (32, 768)\n",
      "Tamaño: 96.0 KB\n",
      "Tipo: float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------- SANITY CHECK: VALIDACIÓN DE EMBEDDINGS GUARDADOS ---------\n",
    "\n",
    "output_dirs = {\n",
    "    'resnet': os.path.join(OUTPUT_DIR_RESNET, 'features_resnet'),\n",
    "    'efficientnet': os.path.join(OUTPUT_DIR_EFFICIENTNET, 'features_efficientnet'),\n",
    "    'vit': os.path.join(OUTPUT_DIR_VIT, 'features_vit')\n",
    "}\n",
    "\n",
    "# 1. CONTEO GENERAL DE ARCHIVOS GENERADOS\n",
    "print(\"\\nVALIDACIÓN DE EMBEDDINGS GUARDADOS:\")\n",
    "total_videos_expected = len(df_global)\n",
    "resnet_files = [f for f in os.listdir(output_dirs['resnet']) if f.endswith('.npy')]\n",
    "efficientnet_files = [f for f in os.listdir(output_dirs['efficientnet']) if f.endswith('.npy')]\n",
    "vit_files = [f for f in os.listdir(output_dirs['vit']) if f.endswith('.npy')]\n",
    "\n",
    "print(f\"Vídeos esperados (CSV): {total_videos_expected}\")\n",
    "print(f\"Embeddings ResNet guardados: {len(resnet_files)}\")\n",
    "print(f\"Embeddings EfficientNet guardados: {len(efficientnet_files)}\")\n",
    "print(f\"Embeddings ViT guardados: {len(vit_files)}\")\n",
    "\n",
    "# 2. VERIFICACIÓN POR DATASET (MELD vs IEMOCAP)\n",
    "print(\"\\nVALIDACIÓN POR DATASET:\")\n",
    "for dataset in ['MELD', 'IEMOCAP']:\n",
    "    dataset_videos = df_global[df_global['dataset_origin'] == dataset]\n",
    "    expected_count = len(dataset_videos)\n",
    "\n",
    "    # Contamos archivos que corresponden a este dataset en concreto\n",
    "    dataset_resnet_files = []\n",
    "    dataset_efficientnet_files = []\n",
    "    dataset_vit_files = []\n",
    "\n",
    "    for _, row in dataset_videos.iterrows():\n",
    "        filename = f\"{row['Dialogue_ID']}_{row['Utterance_ID']}.npy\".replace(\"/\",\"_\")\n",
    "\n",
    "        if filename in resnet_files:\n",
    "            dataset_resnet_files.append(filename)\n",
    "        if filename in efficientnet_files:\n",
    "            dataset_efficientnet_files.append(filename)\n",
    "        if filename in vit_files:\n",
    "            dataset_vit_files.append(filename)\n",
    "\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    print(f\"Vídeos esperados: {expected_count}\")\n",
    "    print(f\"ResNet embeddings: {len(dataset_resnet_files)} ({len(dataset_resnet_files)/expected_count*100:.1f}%)\")\n",
    "    print(f\"EfficientNet embeddings: {len(dataset_efficientnet_files)} ({len(dataset_efficientnet_files)/expected_count*100:.1f}%)\")\n",
    "    print(f\"ViT embeddings: {len(dataset_vit_files)} ({len(dataset_vit_files)/expected_count*100:.1f}%)\")\n",
    "\n",
    "    try:\n",
    "        assert len(dataset_resnet_files) == expected_count, f\"FALLO en {dataset}: ResNet esperaba {expected_count}, encontró {len(dataset_resnet_files)}\"\n",
    "        assert len(dataset_efficientnet_files) == expected_count, f\"FALLO en {dataset}: EfficientNet esperaba {expected_count}, encontró {len(dataset_efficientnet_files)}\"\n",
    "        assert len(dataset_vit_files) == expected_count, f\"FALLO en {dataset}: ViT esperaba {expected_count}, encontró {len(dataset_vit_files)}\"\n",
    "    except AssertionError as e:\n",
    "        print(f\"{dataset}: {str(e)}\")\n",
    "\n",
    "# 3. VERIFICACIÓN POR SPLIT/PARTICIÓN\n",
    "print(\"\\nVALIDACIÓN POR SPLIT/PARTICIÓN:\")\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    split_videos = df_global[df_global['split'] == split]\n",
    "    expected_count = len(split_videos)\n",
    "\n",
    "    if expected_count == 0:\n",
    "        print(f\"{split.upper()}: No hay vídeos en esta partición\")\n",
    "        continue\n",
    "\n",
    "    # Contamos archivos que corresponden a este split\n",
    "    split_resnet_files = []\n",
    "    split_efficientnet_files = []\n",
    "    split_vit_files = []\n",
    "\n",
    "    for _, row in split_videos.iterrows():\n",
    "        filename = f\"{row['Dialogue_ID']}_{row['Utterance_ID']}.npy\".replace(\"/\",\"_\")\n",
    "\n",
    "        if filename in resnet_files:\n",
    "            split_resnet_files.append(filename)\n",
    "        if filename in efficientnet_files:\n",
    "            split_efficientnet_files.append(filename)\n",
    "        if filename in vit_files:\n",
    "            split_vit_files.append(filename)\n",
    "\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(f\"Vídeos esperados: {expected_count}\")\n",
    "    print(f\"ResNet embeddings: {len(split_resnet_files)} ({len(split_resnet_files)/expected_count*100:.1f}%)\")\n",
    "    print(f\"EfficientNet embeddings: {len(split_efficientnet_files)} ({len(split_efficientnet_files)/expected_count*100:.1f}%)\")\n",
    "    print(f\"ViT embeddings: {len(split_vit_files)} ({len(split_vit_files)/expected_count*100:.1f}%)\")\n",
    "\n",
    "    # Assert por split:\n",
    "    try:\n",
    "        assert len(split_resnet_files) == expected_count, f\"FALLO en {split}: ResNet esperaba {expected_count}, encontró {len(split_resnet_files)}\"\n",
    "        assert len(split_efficientnet_files) == expected_count, f\"FALLO en {split}: EfficientNet esperaba {expected_count}, encontró {len(split_efficientnet_files)}\"\n",
    "        assert len(split_vit_files) == expected_count, f\"FALLO en {split}: ViT esperaba {expected_count}, encontró {len(split_vit_files)}\"\n",
    "    except AssertionError as e:\n",
    "        print(f\"{split.upper()}: {str(e)}\")\n",
    "\n",
    "# 4. MOSTRAMOS EJEMPLOS DE EMBEDDINGS GUARDADOS\n",
    "print(\"\\nEJEMPLOS DE EMBEDDINGS GUARDADOS:\")\n",
    "if len(resnet_files) > 0:\n",
    "    example_resnet = resnet_files[0]\n",
    "    resnet_path = os.path.join(output_dirs['resnet'], example_resnet)\n",
    "    resnet_data = np.load(resnet_path)\n",
    "    print(f\"\\nResNet ejemplo: {example_resnet}\")\n",
    "    print(f\"Shape: {resnet_data.shape}\")\n",
    "    print(f\"Tamaño: {resnet_data.nbytes/1024:.1f} KB\")\n",
    "    print(f\"Tipo: {resnet_data.dtype}\")\n",
    "\n",
    "if len(efficientnet_files) > 0:\n",
    "    example_eff = efficientnet_files[0]\n",
    "    eff_path = os.path.join(output_dirs['efficientnet'], example_eff)\n",
    "    eff_data = np.load(eff_path)\n",
    "    print(f\"\\nEfficientNet ejemplo: {example_eff}\")\n",
    "    print(f\"Shape: {eff_data.shape}\")\n",
    "    print(f\"Tamaño: {eff_data.nbytes/1024:.1f} KB\")\n",
    "    print(f\"Tipo: {eff_data.dtype}\")\n",
    "\n",
    "if len(vit_files) > 0:\n",
    "    example_vit = vit_files[0]\n",
    "    vit_path = os.path.join(output_dirs['vit'], example_vit)\n",
    "    vit_data = np.load(vit_path)\n",
    "    print(f\"\\nViT ejemplo: {example_vit}\")\n",
    "    print(f\"Shape: {vit_data.shape}\")\n",
    "    print(f\"Tamaño: {vit_data.nbytes/1024:.1f} KB\")\n",
    "    print(f\"Tipo: {vit_data.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5898b",
   "metadata": {},
   "source": [
    "Se confirma que se han generado correctamente todos los *embeddings* **visuales** (este paso es imprescindible para construir la base de nuestros modelos posteriores, ya que estas serán las entradas visuales que recibirán inicialmente). Por tanto, contamos con **21219** embeddings visuales obtenidos con **ResNet50** de tamaño `(32,2048)`, otros **21219** obtenidos con **EfficientNet** de tamaño `(32,1280)`, y **21219** obtenidos con **ViT** de tamaño `(32,768)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c1a22e",
   "metadata": {},
   "source": [
    "## **Extracción de Características Textuales (Textual Embeddings)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb59a8",
   "metadata": {},
   "source": [
    "En este notebook abordamos la fase de extracción de características para la modalidad de texto (transcripciones de diálogos). Mientras que el audio captura el \"cómo\" se dice y el vídeo el \"gesto\" con el que se dice, el texto aporta la semántica y el contexto lingüístico, imprescimdibles en nuestra tarea de detección de **Estrés**.\n",
    "\n",
    "Al igual que en las modalidades anteriores, no utilizaremos el texto en crudo, sino que emplearemos modelos de lenguaje basados en Transformers para generar embeddings contextuales. A diferencia de técnicas tradicionales como Word2Vec, estos modelos utilizan mecanismos de *self-attention* que permiten que la representación de cada palabra dependa de todas las demás palabras de la frase, capturando matices de estrés y frustración.\n",
    "\n",
    "Dado que todas las transcripciones de nuestro dataset se encuentran en inglés, se han seleccionado los siguientes modelos preentrenados específicamente en este idioma para realizar un estudio comparativo:\n",
    "\n",
    "1. **BERT** : Es utilizado como *baseline*. Es el modelo clásico pero efectivo que revolucionó el NLP al procesar el texto de forma bidireccional.\n",
    "\n",
    "2. **RoBERTa**: Versión mejorada de **BERT**, que optimiza el proceso de entrenamiento y el tamaño de los datos, demostrando mayor eficacia en la captura de matices emocionales en diálogos conversacionales.\n",
    "\n",
    "3. **DeBERTa-v3**: Representa el estado del arte actual. Su mecanismo de atención permite separar la información del contenido de la posición de la palabra, lo que permite una mayor capacidad para entender estructuras gramaticales complejas bajo tensión.\n",
    "\n",
    "\n",
    "**NOTA**: Para cada frase, extraeremos los embeddings de la última capa oculta. El resultado será un conjunto de archivos *.npy* con dimensiones $(T, 768)$, donde $T$ es el número de tokens y 768 es la densidad del vector de características (en los 3 modelos).\n",
    "Al cargar la versión **Base** de los tres modelos, se obtienen esas 768 dimensiones, alineadas con los modelos más robustos en audio (**Wav2Vec2.0**) y visión (**ViT**), que convergen de forma nativa a **768 dimensiones** en sus versiones base cargadas. Esto facilita la creación de espacio latente común, donde las dimensiones de entrada están equilibradas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc79eba",
   "metadata": {},
   "source": [
    "### Estrategia de Extracción Multi-Ventana:\n",
    "\n",
    "Basándonos en los resultados obtenidos del EDA, donde se determinó que el **99% de las transcripciones** en los datasets MELD e IEMOCAP poseen una longitud de **27** y **47** tokens respectivamente, se ha diseñado una estrategia de extracción para cumplir con los requisitos del estudio de ablación metodológico:\n",
    "\n",
    "1.  **Ventana Corta ($T=32$ tokens):** Respuesta inmediata y palabras clave. El objetivo es evaluar si el modelo es capaz de detectar el estrés basándose únicamente en el inicio de la intervención o en frases cortas.\n",
    "2.  **Ventana Media ($T=64$ tokens):** Balance estándar, tamaño óptimo extraído del EDA ya que cubre el 100% de las transcripciones en MELD y más del 99% en IEMOCAP, evitando la introducción excesiva de *padding* que sí ocurriría con ventanas mayores (por ejemplo, de 128 o 256) que sería excesivo en nuestro caso.\n",
    "\n",
    "\n",
    "**NOTA**: Para garantizar la validez de la comparación entre ambos tamaños de ventana, se realizan **inferencias independientes** (`max_length=32`, `max_length=64`) para cada configuración. Esto evita el \"ruido contextual\" que se introduciría al recortar artificialmente un tensor generado con una ventana mayor, ya que los mecanismos de *self-attention* de los Transformers (BERT, RoBERTa, DeBERTa) generan dependencias bidireccionales entre todos los tokens procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52559987",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers sentencepiece torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Carga previa de todas las librerías y paquetes necesarios\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Configuración de GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97b29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Se monta Drive para acceder a los datos y guardar los resultados:\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04613a2d",
   "metadata": {},
   "source": [
    "Traemos los datos al disco del servidor de Colab.\n",
    "\n",
    "La estructura de directorios final dónde se almacenarán los resultados en Drive es la siguiente:\n",
    "* **`Proyecto_TFG_Data`**:\n",
    "\n",
    "    * **`EMBEDDINGS_TEXT_BERT`**: \n",
    "\n",
    "        * **`EMBEDDINGS_TEXT_BERT_32`**: Aquí se almacenarán los embeddings de tamaño $(32,768)$ obtenidos de **BERT**.\n",
    "\n",
    "        * **`EMBEDDINGS_TEXT_BERT_64`**: Aquí se almacenarán los embeddings de tamaño $(64,768)$ obtenidos de **BERT**.\n",
    "\n",
    "    * **`EMBEDDINGS_TEXT_ROBERTA`**: \n",
    "\n",
    "        * **`EMBEDDINGS_TEXT_ROBERTA_32`**: Aquí se almacenarán los embeddings de tamaño $(32,768)$ obtenidos de **RoBERTa**.\n",
    "        \n",
    "        * **`EMBEDDINGS_TEXT_ROBERTA_64`**: Aquí se almacenarán los embeddings de tamaño $(64,768)$ obtenidos de **RoBERTa**.\n",
    "\n",
    "    * **`EMBEDDINGS_TEXT_DEBERTA`**: \n",
    "\n",
    "        * **`EMBEDDINGS_TEXT_DEBERTA_32`**: Aquí se almacenarán los embeddings de tamaño $(32,768)$ obtenidos de **DeBERTa**.\n",
    "        \n",
    "        * **`EMBEDDINGS_TEXT_DEBERTA_64`**: Aquí se almacenarán los embeddings de tamaño $(64,768)$ obtenidos de **DeBERTa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dded6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_ROOT = '/content/data' # Directorio local en Colab para almacenar los datos\n",
    "os.makedirs(LOCAL_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "# RUTAS LOCALES para guardar los .npy inicialmente (Disco SSD de Coolab):\n",
    "LOCAL_WORK_DIR = '/content/data_temp_features'\n",
    "if os.path.exists(LOCAL_WORK_DIR):\n",
    "    shutil.rmtree(LOCAL_WORK_DIR) # Limpiar si existe de antes\n",
    "os.makedirs(LOCAL_WORK_DIR, exist_ok=True)\n",
    "\n",
    "# Rutas de SALIDA EN DRIVE:\n",
    "# Los embeddings sí se guardarán en Drive, pero los datos de entrada se procesarán desde el almacenamiento local para mayor eficiencia\n",
    "\n",
    "# BERT:\n",
    "OUTPUT_DIR_BERT = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_TEXT/EMBEDDINGS_TEXT_BERT'  # Directorio en Drive para guardar ambos embeddings textuales extraídos de BERT\n",
    "os.makedirs(OUTPUT_DIR_BERT, exist_ok=True)  # Crear el directorio de salida si no existe\n",
    "\n",
    "# RoBERTa:\n",
    "OUTPUT_DIR_ROBERTA = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_TEXT/EMBEDDINGS_TEXT_ROBERTA'  # Directorio en Drive para guardar ambos embeddings textuales extraídos de RoBERTa\n",
    "os.makedirs(OUTPUT_DIR_ROBERTA, exist_ok=True)  # Crear el directorio de salida si no existe\n",
    "\n",
    "# DeBERTa:\n",
    "OUTPUT_DIR_DEBERTA = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_TEXT/EMBEDDINGS_TEXT_DEBERTA'  # Directorio en Drive para guardar ambos los embeddings textuales extraídos de DeBERTa\n",
    "os.makedirs(OUTPUT_DIR_DEBERTA, exist_ok=True)  # Crear el directorio de salida si no existe\n",
    "\n",
    "# Cargamos el CSV directamente desde Drive:\n",
    "DATA_ROOT_CSV = '/content/drive/MyDrive/Proyecto_TFG_Data/Multimodal_Stress_Dataset.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8af84204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a851c852-e531-48fa-9082-53a2127a6780\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>video_path</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "      <th>target_stress</th>\n",
       "      <th>dataset_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_dia0_utt0</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt0.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt0.wav</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>5.672333</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_dia0_utt1</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt1.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt1.wav</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>1.501500</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_dia0_utt2</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt2.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt2.wav</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>2.919583</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_dia0_utt3</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt3.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt3.wav</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>2.752750</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_dia0_utt4</td>\n",
       "      <td>0</td>\n",
       "      <td>train_splits/dia0_utt4.mp4</td>\n",
       "      <td>MELD_Audio/train_dia0_utt4.wav</td>\n",
       "      <td>My duties? All right.</td>\n",
       "      <td>6.464792</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>MELD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a851c852-e531-48fa-9082-53a2127a6780')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a851c852-e531-48fa-9082-53a2127a6780 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a851c852-e531-48fa-9082-53a2127a6780');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Utterance_ID Dialogue_ID                  video_path  \\\n",
       "0  train_dia0_utt0           0  train_splits/dia0_utt0.mp4   \n",
       "1  train_dia0_utt1           0  train_splits/dia0_utt1.mp4   \n",
       "2  train_dia0_utt2           0  train_splits/dia0_utt2.mp4   \n",
       "3  train_dia0_utt3           0  train_splits/dia0_utt3.mp4   \n",
       "4  train_dia0_utt4           0  train_splits/dia0_utt4.mp4   \n",
       "\n",
       "                       audio_path  \\\n",
       "0  MELD_Audio/train_dia0_utt0.wav   \n",
       "1  MELD_Audio/train_dia0_utt1.wav   \n",
       "2  MELD_Audio/train_dia0_utt2.wav   \n",
       "3  MELD_Audio/train_dia0_utt3.wav   \n",
       "4  MELD_Audio/train_dia0_utt4.wav   \n",
       "\n",
       "                                       Transcription  duration  split  \\\n",
       "0  also I was the point person on my company's tr...  5.672333  train   \n",
       "1                   You must've had your hands full.  1.501500  train   \n",
       "2                            That I did. That I did.  2.919583  train   \n",
       "3      So let's talk a little bit about your duties.  2.752750  train   \n",
       "4                              My duties? All right.  6.464792  train   \n",
       "\n",
       "   target_stress dataset_origin  \n",
       "0              0           MELD  \n",
       "1              0           MELD  \n",
       "2              0           MELD  \n",
       "3              0           MELD  \n",
       "4              0           MELD  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carga del dataset global\n",
    "\n",
    "file_path = DATA_ROOT_CSV\n",
    "if os.path.exists(file_path):\n",
    "    df_global = pd.read_csv(file_path)\n",
    "    display(df_global.head()) \n",
    "else:\n",
    "    print(f\"No se encuentra el archivo en {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3eb8b",
   "metadata": {},
   "source": [
    "----\n",
    "## ***Feed-forward.* Fase de Extracción**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración diccionario con los modelos a utilizar y sus respectivas rutas en Hugging Face:\n",
    "MODELS_CONFIG = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "    \"deberta\": \"microsoft/deberta-v3-base\"\n",
    "}\n",
    "\n",
    "# Longitudes de ventana fijadas:\n",
    "WINDOW_SIZES = [32, 64]\n",
    "\n",
    "def extract_features(df, model_name, hf_path, max_len, local_output_dir):\n",
    "    \"\"\"\n",
    "    Función para extraer características textuales utilizando un modelo de Hugging Face dado.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con las transcripciones y metadatos.\n",
    "        model_name (str): Nombre del modelo (e.g., 'bert', 'roberta', 'deberta').\n",
    "        hf_path (str): Ruta del modelo en Hugging Face.\n",
    "        max_len (int): Longitud máxima de la ventana de texto (tokens)\n",
    "        local_output_dir (str): Directorio local donde se guardarán los embeddings extraídos.\n",
    "    \n",
    "    Devuelve:\n",
    "        None (los embeddings se guardan como archivos .npy en el directorio especificado).\n",
    "    \"\"\"\n",
    "    os.makedirs(local_output_dir, exist_ok=True)  # Crear el directorio de salida si no existe  \n",
    "    #Cargamos el tokenizador y el modelo desde Hugging Face\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "    model = AutoModel.from_pretrained(hf_path).to(device) # Cargamos el modelo en el dispositivo (GPU o CPU)\n",
    "    model.eval()  # Ponemos el modelo en modo evaluación\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Extrayendo características con {model_name} (max_len={max_len})\"):\n",
    "        file_name = f\"{row['Dialogue_ID']}_{row['Utterance_ID']}.npy\".replace(\"/\",\"_\")\n",
    "\n",
    "        file_path = os.path.join(local_output_dir, file_name)\n",
    "\n",
    "        transcription = row['Transcription']\n",
    "\n",
    "        # Tokenización y creación de tensores\n",
    "        inputs = tokenizer(transcription, # Tokenizamos la transcripción\n",
    "                            return_tensors='pt', # Devolvemos tensores PyTorch\n",
    "                            truncation=True,  # Truncamos el texto si excede max_len\n",
    "                            padding='max_length',  # Rellenamos con ceros hasta max_len\n",
    "                            max_length=max_len) # Establecemos la longitud máxima de la ventana\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        # Extracción de características sin calcular gradientes (inferencia):\n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**inputs) # Obtenemos las salidas del modelo (embeddings)\n",
    "            # Guardamos el embedding de la secuencia completa:\n",
    "            embedding = outputs.last_hidden_state.squeeze(0).cpu().numpy() \n",
    "\n",
    "        # Guardar el embedding como un archivo .npy\n",
    "        np.save(file_path, embedding)  # Guardamos el embedding en un archivo .npy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf1351e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859169c471c64204953d1416550b7fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b40d37e4c44a0799fdee8d43cc45ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extrayendo características con bert (max_len=32):   0%|          | 0/21219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880e7e86fceb4489b28bb21b399c6e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
      "cls.predictions.bias                       | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f6faffff5f4a3b8cf186933a1ed7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extrayendo características con bert (max_len=64):   0%|          | 0/21219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e9cd09f92a4ab3a4df49711621fe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a478a74601a742b79d7435de7dfb828e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae6de7560a74ab3b58e24191fe3df1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace011ceda264ffba36b20e174a9396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29a31ed57da48ff82d0cd8575d2fc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1403412946433e8bbf7bcd33a65393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd5b192fd2147b9aa2254bfbe2368d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaModel LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc25022c00d14318a7ee4db34760321e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extrayendo características con roberta (max_len=32):   0%|          | 0/21219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869a05948e494da788042fae585c127d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaModel LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a32d9ab4fb84a6a992a21ad7d183753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extrayendo características con roberta (max_len=64):   0%|          | 0/21219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5269fa3226744bcb6fac1970e36b7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2177a4e296419c913171282c2e6074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe5d9c0b0dd4bd08c89daca1e5ef93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174161e5b0e14a0293ba5b86a51d57d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667dd588bb6494c9abba7bb899f5a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Model LOAD REPORT from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb4a05909a4401ea5606ad8333d3311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extrayendo características con deberta (max_len=32):   0%|          | 0/21219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db70b6d45407466fa5cd9f313af9be62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde2fac199064b71a5a1e7d4e57e79a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DebertaV2Model LOAD REPORT from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     |  | \n",
      "----------------------------------------+------------+--+-\n",
      "mask_predictions.dense.weight           | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED |  | \n",
      "mask_predictions.classifier.bias        | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED |  | \n",
      "mask_predictions.dense.bias             | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED |  | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED |  | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED |  | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f604ecd69eb4e5a8eb71610ddaeaf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extrayendo características con deberta (max_len=64):   0%|          | 0/21219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bucle principal:\n",
    "\n",
    "for name, hf_path in MODELS_CONFIG.items():\n",
    "    for max_len in WINDOW_SIZES:\n",
    "        # Definimos el ID de la tarea (ej: bert_32):\n",
    "        task_id = f\"{name}_{max_len}\"\n",
    "\n",
    "        zip_filename = f\"EMBEDDINGS_TEXT_{name.upper()}_{max_len}\"\n",
    "\n",
    "        # Ruta local temporal:\n",
    "        local_save_path = os.path.join(LOCAL_WORK_DIR, task_id)\n",
    "\n",
    "        if name == 'bert':\n",
    "            output_dir = OUTPUT_DIR_BERT \n",
    "        elif name == 'roberta':\n",
    "            output_dir = OUTPUT_DIR_ROBERTA\n",
    "        elif name == 'deberta':\n",
    "            output_dir = OUTPUT_DIR_DEBERTA\n",
    "        \n",
    "        extract_features(df_global, name, hf_path, max_len, local_save_path)\n",
    "\n",
    "        # Una vez extraídos los embeddings en el almacenamiento local, los movemos a Drive:\n",
    "        # Con shutil.make_archive creamos el zip:\n",
    "        shutil.make_archive(f\"/content/{zip_filename}\", 'zip', local_save_path)\n",
    "        shutil.copy(f\"/content/{zip_filename}.zip\", output_dir)\n",
    "\n",
    "        # Eliminamos los archivos temporales locales para liberar espacio:\n",
    "        os.remove(f\"/content/{zip_filename}.zip\")\n",
    "        shutil.rmtree(local_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b81920",
   "metadata": {},
   "source": [
    "### **VALIDACIÓN FINAL DE INTEGRIDAD DE DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL DE FRASES ESPERADAS (según CSV): 21219\n",
      "ARCHIVO ZIP                              | ENCONTRADOS  | ESTADO    \n",
      "EMBEDDINGS_TEXT_BERT_32.zip              | 21219        | OK\n",
      "EMBEDDINGS_TEXT_BERT_64.zip              | 21219        | OK\n",
      "EMBEDDINGS_TEXT_ROBERTA_32.zip           | 21219        | OK\n",
      "EMBEDDINGS_TEXT_ROBERTA_64.zip           | 21219        | OK\n",
      "EMBEDDINGS_TEXT_DEBERTA_32.zip           | 21219        | OK\n",
      "EMBEDDINGS_TEXT_DEBERTA_64.zip           | 21219        | OK\n",
      "\n",
      "Todos los archivos coinciden exactamente con el dataset.\n"
     ]
    }
   ],
   "source": [
    "# Nos aseguramos primero que Drive esté montado\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 1. OBTENEMOS EL NÚMERO \"GROUND TRUTH\" (LO ESPERADO)\n",
    "DATA_ROOT_CSV = '/content/drive/MyDrive/Proyecto_TFG_Data/Multimodal_Stress_Dataset.csv'\n",
    "\n",
    "if os.path.exists(DATA_ROOT_CSV):\n",
    "    df_global = pd.read_csv(DATA_ROOT_CSV)\n",
    "    EXPECTED_COUNT = len(df_global)\n",
    "    print(f\"TOTAL DE FRASES ESPERADAS (según CSV): {EXPECTED_COUNT}\")\n",
    "else:\n",
    "    print(f\"No encuentro el CSV en {DATA_ROOT_CSV}\")\n",
    "    EXPECTED_COUNT = 0\n",
    "\n",
    "# 2. DEFINIMOS LA ESTRUCTURA A VALIDAR\n",
    "# Rutas base donde hemos guardado los zips\n",
    "BASE_DRIVE = '/content/drive/MyDrive/Proyecto_TFG_Data/EMBEDDINGS_TEXT'\n",
    "MODELS = [\"BERT\", \"ROBERTA\", \"DEBERTA\"]\n",
    "WINDOW_SIZES = [32, 64]\n",
    "\n",
    "\n",
    "print(f\"{'ARCHIVO ZIP':<40} | {'ENCONTRADOS':<12} | {'ESTADO':<10}\")\n",
    "\n",
    "\n",
    "all_success = True\n",
    "\n",
    "# 3. BUCLE DE VERIFICACIÓN\n",
    "for model in MODELS:\n",
    "    # Ruta de la carpeta del modelo (ej: .../EMBEDDINGS_TEXT_BERT)\n",
    "    model_dir = os.path.join(BASE_DRIVE, f\"EMBEDDINGS_TEXT_{model}\")\n",
    "    \n",
    "    for size in WINDOW_SIZES:\n",
    "        # Nombre del ZIP (ej: EMBEDDINGS_TEXT_BERT_32.zip)\n",
    "        zip_name = f\"EMBEDDINGS_TEXT_{model}_{size}.zip\"\n",
    "        zip_full_path = os.path.join(model_dir, zip_name)\n",
    "        \n",
    "        found_count = 0\n",
    "        status = \"MISSING\"\n",
    "        \n",
    "        if os.path.exists(zip_full_path):\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_full_path, 'r') as zip_ref:\n",
    "                    # Filtramos solo archivos .npy \n",
    "                    file_list = [f for f in zip_ref.namelist() if f.endswith('.npy')]\n",
    "                    found_count = len(file_list)\n",
    "                \n",
    "                if found_count == EXPECTED_COUNT:\n",
    "                    status = \"OK\"\n",
    "                else:\n",
    "                    status = f\" DIFERENCIA ({EXPECTED_COUNT - found_count})\"\n",
    "                    all_success = False\n",
    "            except zipfile.BadZipFile:\n",
    "                status = \"CORRUPTO\"\n",
    "                all_success = False\n",
    "        else:\n",
    "            all_success = False\n",
    "\n",
    "        print(f\"{zip_name:<40} | {found_count:<12} | {status}\")\n",
    "\n",
    "\n",
    "if all_success:\n",
    "    print(\"\\nTodos los archivos coinciden exactamente con el dataset.\")\n",
    "else:\n",
    "    print(\"\\nSe encontraron discrepancias.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
